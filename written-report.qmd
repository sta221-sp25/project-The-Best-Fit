---
title: "Characterizing and Predicting Article Virality based off of Metadata and Article Attributes"
author: "The BEST Fit - Philip, Olivia, Leo, Allison"
date: "4/28/2025"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load-pkg-data

library(pROC)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(grid) 
library(knitr)
library(car)
library(patchwork)


newsdf <- read_csv("data/OnlineNewsPopularity.csv")
```

## **Introduction and Data**

#### [**Project Motivation and Research Question**]{.underline}

The ways in which people interact with media and discover news have dramatically shifted in recent years, with social media often displacing traditional news outlets. The decentralized nature of social media means the reach of each article is largely dependent on its individual merits, rather than the popularity of the publication it belongs to. Thus, we are interested in exploring what exactly impacts an article's virality and can these factors aid news agencies when publishing new articles. Our research question is as follows: What article attributes are associated with social media virality?

#### [**Dataset and Key Variables**]{.underline}

In this report we investigate the effects of different article features on social media success using the University of California Irvine Machine Learning Repository’s “Online News Popularity” data set. It includes share counts and descriptive characteristics for articles published by Mashable, a digital media website, over two years (from 2013 to 2015). The data has 39,644 entries in total, with each representing an individual article and its associated textual and metadata features.

**Key Variables:**

**is_viral** - Binary response variable we created to evaluate whether an article is considered viral or not, based on whether total shares is greater/less than 1,400 based on past literature (Fernandes Et. al 2015) (0: FALSE, 1: TRUE).

**data_channel** - Categorical variable denoting article topic, merged from indicators: `data_channel_is_lifestyle`, `data_channel_is_entertainment`, `data_channel_is_bus`, `data_channel_is_socmed`, `data_channel_is_tech`, and `data_channel_is_world`. This classifies content by subject area.

**day_published** - Categorical variable indicating publication day, merged from indicators: `weekday_is_monday`, `weekday_is_tuesday`, `weekday_is_wednesday`, `weekday_is_thursday`, `weekday_is_friday`, `weekday_is_saturday`, `weekday_is_sunday`. Additionally includes `is_weekend` (mean 0.1309) to distinguish weekday from weekend publications.

**title_sentiment_polarity** - Measure of the title's sentiment polarity (positivity/negativity). Values range from -1.0 (extremely negative) to 1.0 (extremely positive), with a mean of 0.0714 and standard deviation of 0.2654. This indicates how emotionally charged article titles are.

**n_tokens_content** - Number of words in the article content. Values range from 0 to 8,474 words, with a mean of 546.51 and standard deviation of 471.10. This quantifies the overall length of the article.

**kw_avg_avg** - Average shares of average keywords in the article. Values range from 0.0 to 43,567.66, with a mean of 3,135.86 and standard deviation of 1,318.13. This measures the expected popularity of the article's keyword selection.

**global_subjectivity** - Measures the overall subjectivity of the article text. Values range from 0.0 (completely objective) to 1.0 (completely subjective), with a mean of 0.4434 and standard deviation of 0.1167. This quantifies how opinion-based versus fact-based the content is.

#### [**Univariate EDA**]{.underline}

To understand our response and predictor variables more deeply, we first looked at their individual distributions. We found that while some variables are relatively approximately symmetric, others are heavily skewed and required log transformations to better meet modeling assumptions.

```{r}
#| label: sentiment-subjectivity-visuals
#| fig-width: 6
#| fig-height: 3
install.packages("gridExtra")
p1 <- ggplot(newsdf, aes(x = title_sentiment_polarity)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
   labs(title = "Dist. of Title Sentiment",
       x = "Title Sentiment Score",
       y = "Count of Articles") +
  theme_minimal()

p2 <- ggplot(newsdf, aes(x = global_subjectivity)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
   labs(title = "Dist. of Article Subjectivity",
       x = "Subjectivity Score",
       y = "Count of Articles") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

`title_sentiment_polarity`'s distribution suggests that many titles are emotionally neutral. The `global_subjectivity` variable has a roughly symmetric distribution, suggesting most articles contained a balanced mix of factual and opinion-based language. Both display relatively balanced distributions that do not require transformation. `kw_avg_avg` also has a relatively symmetric distribution, and was left untransformed (re: fig 1, appendix).

```{r}
#| label: length-visual 
#| fig-width: 7
#| fig-height: 3
p5 <- ggplot(newsdf, aes(x = n_tokens_content)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
   labs(title = "Dist. of Article Length",
       x = "Word Count",
       y = "Count of Articles") +
  theme_minimal()

p6 <- ggplot(newsdf, aes(x = log(n_tokens_content))) +  
  geom_histogram(fill = "purple", color = "black", alpha = 0.7) +
  labs(title = "Dist. of Article Length (Log-Trans.)",
       x = "Word Count (Log-trans.)",
       y = "Count of Articles")+
  theme_minimal()

grid.arrange(p5, p6, ncol = 2)
```

`n_tokens_content` displayed a heavily right-skewed distributions that warranted log transformations. To correct for this skewness and reduce the influence of extreme values, we log-transformed it, resulting in a more centered and symmetric distribution. better aligning with modeling assumptions.

[**Bivariate EDA**]{.underline}

```{r}
#| label: data-df-setup
newsdf$is_viral <- ifelse(newsdf$shares >= 1400, 1, 0)

newsdf$data_channel <- NA

newsdf$data_channel <- case_when(
  newsdf$data_channel_is_entertainment == 1 ~ "Entertainment",
  newsdf$data_channel_is_bus == 1 ~ "Business",
  newsdf$data_channel_is_socmed == 1 ~ "Social Media",
  newsdf$data_channel_is_tech == 1 ~ "Technology",
  newsdf$data_channel_is_world == 1 ~ "World",
)

newsdf$data_channel <- factor(newsdf$data_channel)
```

```{r}
#| label: day-df-setup
newsdf$day_published <- NA  # Create a new empty column

newsdf$day_published <- case_when(
  newsdf$weekday_is_monday == 1 ~ "Monday",
  newsdf$weekday_is_tuesday == 1 ~ "Tuesday",
  newsdf$weekday_is_wednesday == 1 ~ "Wednesday",
  newsdf$weekday_is_thursday == 1 ~ "Thursday",
  newsdf$weekday_is_friday == 1 ~ "Friday",
  newsdf$weekday_is_saturday == 1 ~ "Saturday",
  newsdf$weekday_is_sunday == 1 ~ "Sunday",
)

newsdf$day_published <- factor(
  newsdf$day_published,
  levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
)
```

```{r}
#| label: day-and-data-channel-vs-viral-plot
#| fig-width: 8
#| fig-height: 3

day_published_plot <- newsdf |>
  filter(!is.na(day_published)) |>
  group_by(day_published, is_viral) |>
  summarise(n = n(), .groups = "drop") |>
  group_by(day_published) |>
  mutate(prop = n / sum(n)) |>
  ggplot(aes(x = day_published, y = prop, fill = factor(is_viral))) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Day Published",
       y = "Proportion",
       fill = "Article Type") +   
  coord_flip() +
  scale_fill_discrete(labels = c("Not Viral", "Viral")) + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "bottom")


data_channel_plot <- newsdf |>
  filter(!is.na(data_channel)) |>
  group_by(data_channel, is_viral) |>
  summarise(n = n(), .groups = "drop") |>
  group_by(data_channel) |>
  mutate(prop = n / sum(n)) |>
  ggplot(aes(x = data_channel, y = prop, fill = factor(is_viral))) +
  geom_col(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Data Channel",
       y = "Proportion",
       fill = "Article Type") +   
  coord_flip() +
  scale_fill_discrete(labels = c("Not Viral", "Viral")) + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "bottom")

(data_channel_plot | day_published_plot) +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "Proportion of Viral Articles by Channel and Day",
    theme = theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
                  legend.position = "bottom")
  )

```

The first graph shows how virality varies across different content categories, described by data_channel. Articles published in the Social Media category have the highest proportion of viral articles, compared to other categories. This suggests that content tailored for or about social platforms may be particularly beneficial for engagement. Articles that are categorized under Technology and Business also show strong performance, with over half of the articles in each channel categorized as viral. On the other hand, articles in the Entertainment and World categories had a lower proportion of viral articles, falling below the 50% mark. This graph underscores how topic area potentially influences content reach due to differences in audience behavior or platform algorithms.

The second graph examines the relationship between the day an article is published and its likelihood of going viral. Articles published on weekends are substantially more likely to be viral compared to those published during the week. Saturday stands out with the highest proportion of viral content, followed closely by Sunday. In contrast, weekday articles tend to have lower virality rates, with viral and non-viral occurring in nearly equal proportions. These findings suggest that timing plays a role in determining an article's reach, likely reflecting differences in user engagement patterns across the week.

```{r}
#| label: vis-interact-effects 
#| fig-width: 9 
#| fig-height: 5

newsdf_mod <- newsdf |>
  filter(!is.na(data_channel))

 interactData <- ggplot(newsdf_mod, aes(fill=factor(is_viral), y=global_subjectivity)) + 
   geom_boxplot(position = position_dodge()) + 
   facet_wrap(~ data_channel) + 
   coord_flip(ylim = c(0, 0.6)) + 
   scale_fill_discrete(labels = c("Not Viral", "Viral")) + 
   labs(
     y = "Global Subjectivity", 
     fill = "Virality"
   )
 
 interactContent <- ggplot(newsdf_mod, aes(fill=factor(is_viral), y=log(n_tokens_content))) + 
   geom_boxplot(position = position_dodge(width = 0.8)) + 
   facet_wrap(~ data_channel) + 
   coord_cartesian(ylim = c(-0.1, 0.1)) + 
   coord_flip(ylim = c(4, 8)) +
   scale_fill_discrete(labels = c("Not Viral", "Viral")) + 
   labs(
     y = "Article Body Length (log transformed)", 
     fill = "Virality"
   )
 
 (interactData | interactContent) +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "Article Virality by Channel and Subjectivity/Length",
    theme = theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
                  legend.position = "bottom")
  )
```

From visualizations of global subjectivity/(log) article body length compared to virality proportions and proportioned by topic area, we also found two potential interaction effects we wanted to consider when building our model, as the relationship between global subjectivity/(log) article length and virality differed across data channel type.

```{r}
#| label: emp-logit 
#| fig-width: 12
#| fig-height: 4

logistic_model <- glm(is_viral ~ kw_avg_avg + 
                        log(n_tokens_content + 0.0001) + 
                        data_channel +
                        day_published + 
                        global_subjectivity +
                        title_sentiment_polarity, 
                      family = binomial(link = "logit"),
                      data = newsdf)

log_odds <- predict(logistic_model, newsdf)
newsdf <- newsdf |>
  bind_cols(log_odds = log_odds)
newsdf <- newsdf |>
  mutate(
    predict_prob = exp(log_odds) / (1 + exp(log_odds))
  )

p11 <- ggplot(data = newsdf, aes(x = kw_avg_avg, y = log_odds)) +
  geom_point(size = 3, color = "blue") +  
  geom_smooth(method = "lm", se = FALSE, color = "red") +  
  labs(
    x = "Keyword Popularity",
    y = "Empirical Logit"
  ) +
  theme_minimal()
p12<- ggplot(data = newsdf, aes(x = global_subjectivity, y = log_odds)) +
  geom_point(size = 3, color = "blue") +  
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Fit a line
  labs(
    x = "Overall Subjectivity",
    y = "Empirical Logit"
  ) +
  theme_minimal()
p13<- ggplot(data = newsdf, aes(x = title_sentiment_polarity, y = log_odds)) +
  geom_point(size = 3, color = "blue") + 
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Fit a line
  labs(
    x = "Title's Sentiment Polarity",
    y = "Empirical Logit"
  ) +
  theme_minimal()

grid.arrange(p11, p12, p13, ncol = 3,
              top = textGrob("Empirical Logit Plot", gp=gpar(fontsize=20, font=2)))
```

The empirical logit plots offer further insight into the relationship between key predictors and the binary response, `is_viral`. The plot for average shares of average keywords reveals a linear positive relationship with log odds of virality. As the average popularity of an article's keywords increases, the log odds of the article going viral increases, with the empirical logit showing an upward trend. This suggests that keyword selection plays a key role in driving article engagement and affirms kw_avg_avg is a key predictor in modeling vitality. In contrast, the plot of overall subjectivity indicated a slight linear positive relationship. Although the empirical logit fit line trends upward, the data points are widely dispersed, and the effect appears weak and inconsistent, implying subjectivity alone is not a reliable driver of viral outcomes. Similarly, the plot for title sentiment polarity shows a marginal positive slope, implying that more positive article titles may be associated with slightly higher chances of virality. However, the relationship is again weak and shows considerable variability. Overall, the empirical logit plots highlight meaningful differences in predictive strength across variables and suggest that keyword popularity metrics are stronger predictors of article virality than emotional tone or subjectivity.

### **Methodology**

While we initially fitted a linear regression model, using log(shares) as our response variable (re: fig 2, appendix), we found that it violated the linear regression assumption of constant variance (re: fig 3, appendix). Thus, based on our initial EDA and empirical logit visualization, we selected data channel, day published, article subjectivity, title sentiment polarity, average keyword popularity, and (log) article content length to fit an initial logistic model instead, shown below:

$$
\begin{aligned}
\log\left( \frac{\pi}{1-\pi} \right) =\ & \beta_0 + \beta_1 \text{kw avg avg} + \beta_2 \log(\text{n tokens content} + 10^{-4}) + \beta_3 \text{global subjectivity}\\ & + \beta_4 \text{title sentiment polarity} \\& + \left( \sum_{i=1}^{4} \beta_{4+i}\ \mathbf{1}\{\text{data channel} = \text{level}_i\} \right) \\& + \left( \sum_{j=1}^{6} \beta_{8+j}\ \mathbf{1}\{\text{day published} = \text{level}_j\} \right)\end{aligned}
$$ All the predictors in our initially fit model possess significant p-values (p\<0.05) and most predictors have relatively high magnitude z-statistics, indicating that all variables in the model have statistically significant relationships with the likelihood of content going viral (re: fig 4, appendix).

#### Coefficient Analysis

When fitting our initial model, we also visualized the adjusted odds ratios to ensure that all predictors were statistically significant.

```{r}
#| label: coeff-estimates 
#| fig-width: 6
#| fig-height: 3

model_odds_ratios <- tidy(logistic_model, exponentiate = TRUE, conf.int = TRUE)

ggplot(data = model_odds_ratios, aes(x = term, y = estimate)) +
  geom_point() +
  geom_hline(yintercept = 1, lty = 2) + 
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high))+
  labs(title = "Adjusted odds ratios",
       x = "",
       y = "Estimated AOR") +
  coord_flip()
```

From this initial visualization, most of the 95% confidence intervals for our predictor coefficients did not include 1, suggesting that the majority of our predictors significantly contributed to the model fit. While a couple predictors (such as day_published_Thursday, kw_avg_avg, and log(n_tokens_content + 0.0001) were close to 1, we kept them in the model due to the overall significance of the categorical variable (ex. day_published) or their role in potential interaction effects.

#### Interaction Effects

Next, we considered the addition of potential interaction effects between article length and data channel, and between global subjectivity and data channel based on our EDA. The hypothesis for this test was:

$$
\begin{aligned}
&H_o : \beta_{n-tokens-content *data-channel } = \beta_{global-subectivity *data-channel } = 0 \\ 
&H_A: \beta_{j} \neq0 \text{ for atleast one }j
\end{aligned}
$$

```{r}
#| label: interaction-effect-eval 
#| echo: false
#| message: false
#| warning: false

interactsMod <- glm(is_viral ~ kw_avg_avg + 
                      log(n_tokens_content+0.0001) + 
                      data_channel +
                      day_published + 
                      global_subjectivity +
                      title_sentiment_polarity + 
                      log(n_tokens_content+0.0001) * data_channel + 
                      global_subjectivity * data_channel, 
                   family = binomial(link = "logit"),
                   data = newsdf)

# Extract log-likelihoods
L_0 <- glance(logistic_model)$logLik
L_a <- glance(interactsMod)$logLik

# Calculate test statistic
G <- -2 * (L_0 - L_a)

# Calculate p-value
p_value <- pchisq(G, df = 5, lower.tail = FALSE)

# Create table for drop in deviance test
deviance_table <- tibble(
  Model = c("Null Model", "Interaction Model"),
  `Log-Likelihood` = c(L_0, L_a),
  `Deviance Statistic (G)` = c(NA, G),
  df = c(NA, 5),
  `p-value` = c(NA, p_value)
)

# Display table using knitr::kable
knitr::kable(deviance_table, 
             caption = "Drop in Deviance Test Results",
             digits = c(0, 3, 3, 0, 4))

```

Examining the output of the deviance test, the p-value is very low, at around 0. This indicates that the data provides sufficient evidence that at-least one of the newly added interaction terms is a statistically significant predictor in whether an article will go viral or not, after accounting for data channel, day published, global subjectivity, title sentiment polarity, average keyword popularity, and main body length for a given article. This suggests that these interaction effects add to our final model.

**Model Evaluation and Comparison**

To reaffirm this model selection, we further compared our two models' ROC curves and AUCs.

```{r}
#| label: data-preparation-function

# --- Data Preparation Function ---
prepare_roc_data <- function(model, data, model_name) {
  probs <- predict(model, type = "response")

  aug_data <- augment(model) |>
    mutate(
      is_viral = as.factor(.data$is_viral),
      probability = probs,
      model = model_name
    )

  return(aug_data)
}

aug_null <- prepare_roc_data(logistic_model, newsdf, "Initial Model")
aug_interact <- prepare_roc_data(interactsMod, newsdf, "Interaction Model")


combined_aug <- bind_rows(aug_null, aug_interact)

# ROC Curve Data Generation
roc_data_null <- aug_null |>
  roc_curve(is_viral, probability, event_level = "second")

roc_data_interact <- aug_interact |>
  roc_curve(is_viral, probability, event_level = "second")
```

```{r}
#| label: auc-calculation
# --- AUC Calculation ---
auc_null <- roc_auc(
  data = filter(combined_aug, model == "Initial Model"),
  truth = is_viral,
  probability,
  event_level = "second"
)

auc_interact <- roc_auc(
  data = filter(combined_aug, model == "Interaction Model"),
  truth = is_viral,
  probability,
  event_level = "second"
)

auc_values <- bind_rows(
  mutate(auc_null, model = "Initial Model"),
  mutate(auc_interact, model = "Interaction Model")
)
```

```{r}
#| label: roc-curve-plotting
#| fig-width: 8
#| fig-height: 4
# --- ROC Curve Plotting: Initial Model ---
plot_null <- ggplot(roc_data_null, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.2, color = "#E41A1C") +
  geom_abline(lty = 2, alpha = 0.5, slope = 1, intercept = 0) +
  coord_equal() +
  labs(
    title = "Initial Model",
    subtitle = paste("AUC =", round(filter(auc_values, model == "Initial Model")$.estimate, 4)),
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()
plot_interact <- ggplot(roc_data_interact, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.2, color = "#4DAF4A") +
  geom_abline(lty = 2, alpha = 0.5, slope = 1, intercept = 0) +
  coord_equal() +
  labs(
    title = "Interaction Model",
    subtitle = paste("AUC =", round(filter(auc_values, model == "Interaction Model")$.estimate, 4)),
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()

grid.arrange(plot_null, plot_interact,
             ncol = 2,
             top = "ROC Curves Comparison")
```

From the ROC curves, we can see that 1) Both models have an ROC curve above the random threshold, approaching the top left corner, indicating some predictive power in classifying an article and 2) The Interaction Model (AUC = 0.6968) demonstrates marginally better predictive performance than the Initial Model (AUC = 0.6915), confirming our belief that the interaction effects are meaningful predictors. 3) Based on the curve, the optimal threshold for our model should target sensitivity \~ 0.65.

Selecting the point closest to the ROC curve to sensitivity 0.65 yields a threshold of approximately 0.493, to be used when evaluating the final model (view appendix).

#### Results

The final model we fit is shown below, with specific coefficient values included in the appendix (re: fig 5, appendix). 

$$
\begin{aligned}
\log\left( \frac{\pi}{1-\pi} \right) =\ & \beta_0 + \beta_1\ \text{kw avg avg} + \beta_2\ \log(\text{n tokens content} + 10^{-4}) + \beta_3\ \text{global subjectivity} \\ & + \beta_4\ \text{title sentiment polarity} \\
& + \left( \sum_{i=1}^{4} \beta_{4+i}\ \mathbf{1}\{\text{data channel} = \text{level}_i\} \right)  + \left( \sum_{j=1}^{6} \beta_{8+j}\ \mathbf{1}\{\text{day published} = \text{level}_j\} \right) \\
& + \left( \sum_{i=1}^{4} \beta_{14+i}\ \log(\text{n tokens content} + 10^{-4})\ \mathbf{1}\{\text{data channel} = \text{level}_i\} \right) \\
& + \left( \sum_{i=1}^{4} \beta_{18+i}\ \text{global subjectivity}\ \mathbf{1}\{\text{data channel} = \text{level}_i\} \right)
\end{aligned}
$$

```{r}
#| fig.cap: "Confusion Matrix for the Interaction Model"
#| fig-width: 4
#| fig-height: 2
# Predict log-odds using the interaction model
log_odds_int <- predict(interactsMod, newdata = newsdf)
news_eval <- newsdf |>
  bind_cols(log_odds_int = log_odds_int)
# Convert to predicted probabilities
news_eval <- news_eval |>
  mutate(
    prob_int = exp(log_odds_int) / (1 + exp(log_odds_int))
  )
news_eval <- news_eval |>
  mutate(
    predicted_label = case_when(
      prob_int < 0.493 ~ 0,
      prob_int >= 0.493 ~ 1,
      TRUE ~ NA_real_
    ),
    predicted_label = as.factor(predicted_label),
    is_viral = as.factor(is_viral)
  )
conf_matrix_int <- news_eval |>
  conf_mat(is_viral, predicted_label)
autoplot(conf_matrix_int, type = "heatmap")
```

When evaluating our final model, we see it has an AUC of $\sim 0.697$, an accuracy of $\sim 64.7$%, specificity of $\sim 64.3$%, and sensitivity of $\sim 65.0$% (re: fig 6). If we consider the misclassification rate of only $\sim 35.3$%, evaluation suggests that our model is moderately well-fit for the data. This precision means that $\sim 65.3$% of articles predicted to be viral were correctly classified, indicating that the model performs substantially better than random chance.

If we assess the assumptions of logistic regression underlying our model, we see approximately linear relationships randomly dispersed around the trend line. Notably, vertical scatter of points around the trend line for subjectivity and title polarity presents a high degree of noise that might be impacting model performance.

Interpreting the coefficients of the model, we can conclude:

Article category, or 'Data Channel' the most strongly correlated with virality, with the odds of articles in the Social Media and Technology categories to go viral are approximately 15.66 times and 4.76 times that of a similar article in the Business category (reference group). Similarly, Entertainment and World news articles also show significantly higher odds of going viral than similar Business news articles, with odds ratios of 2.44 and 2.20, respectively. This suggests that readers are particularly engaged with content about social media and technology innovations, and also tend to share Entertainment and World news more than articles about Business news, however, not to the degree of Social Media and Technology articles.

Day of publication is another important factor, with weekend publications significantly outperforming weekday content. Compared to Monday, Saturday articles have 2.81 times the odds, and Sunday articles have 2.47 times the odds of going viral, holding all else constant. In contrast, the odds of Tuesday and Wednesday articles going viral are 11.7% and 13.5% lower than similar Monday articles, with odds ratios of 0.883 and 0.865, respectively. This weekend effect may arise from increased leisure time, as people take off from work or school during the weekend.

Article subjectivity and sentiment likely also influence virality. A fully subjective article (global subjectivity = 1) has 6.12 times the odds of going viral compared to a fully objective article (global subjectivity = 0), keeping all else constant. Similarly, a one-unit increase in title sentiment polarity (a neutral article compared to strongly positive) increases the odds of virality by 26.6%. This trend supports the idea that emotionally charged or opinionated content tends to be shared more frequently than neutral content.

Keyword popularity also plays a role in determining the odds of an article going viral. Specifically, every time keyword popularity is doubled, the odds of an article going viral increase by about 29.1%, holding all else constant. While article length is a statistically significant predictor of an article’s virality, its impact is minimal. Specifically, every 10% increase in article length increases the odds of virality by approximately 1.55%, holding all else constant. Thus, while article length still adds to our model, it doesn’t add as much as predictors like day published or data channel.

For Entertainment and World news, the benefit of article length is diminished or even reversed. For Entertainment articles, every 10% increase in article length leads to a 1.9% decrease in the odds of going viral compared to a similar Business article, while for World news, a 10% increase in article length results in a 2.5% decrease in odds. The interaction terms between Article Length and Social Media or Technology were not statistically significant, suggesting that the impact of Article Length on probability of virality does not differ greatly for Social Media and Technology compared to the baseline catagory, Business.

In Social Media articles, a fully subjective tone actually reduces the odds of virality by 98.3%, despite the strong main effect of subjectivity. This suggests that objective tone may be more prone to virality for Social Media. In comparison, for Technology articles, each one unit subjectivity increase reduces the odds by 82.6%. However, for both Entertainment and World news, these interaction effects are not statistically significant, which suggests that subjectivity could still hold a positive or neutral effect.

#### Discussion + Conclusion

Our findings support the idea that Article Length, Category, Keyword Popularity, Date of publication, subjectivity and title polarity all contribute to an online article's success, while other factors like the rate of positive/negative words were less reliable predictors. Through the empirical logit plot and the drop-in-deviance test of the interaction terms, we determined that the effects of article length and global subjectivity both vary based on article topics.

Since the Mashable repository was collected over a multi-year time period, one limitation of our approach is the distinct possibility of outside temporal factors. This poses a potential violation of the independent observation assumption, with major events potentially shaping article performance on shorter time scales.

Another limitation to our findings is the poor interpretability of the NLP derived predictors, such as `title-sentiment-polarity` and `keyword average average`, which were derived in an extensive process by the UCI researchers who curated the dataset.

Finally, our model only contains observations from Mashable, so extrapolation to other news/article sites may be limited.

To reduce the influence of these factors in future work, we might control for time period in the data and compare articles between different sites for better generalization. Beyond that, utilizing our own NLP metrics might give us finer grain control, allowing us to consider better predictors for our model.

### Citations

Fernandes, K., Vinagre, P., Cortez, P., & Sernadela, P. (2015). Online News Popularity \[Dataset\]. UCI Machine Learning Repository. https://doi.org/10.24432/C5NS3V.

Fernandes, Kelwin et al. “A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News.” Portuguese Conference on Artificial Intelligence (2015).

Obiedat, R. (2020). Predicting the popularity of online news using classification methods with feature filtering techniques. Journal of Theoretical and Applied Information Technology, 98(8), 1163–1172. http://www.jatit.org

### Appendix

#### Figure 1: Average Keyword Distribution

```{r}
#| label: kw-avg-visual 

ggplot(newsdf, aes(x = kw_avg_avg)) +
  geom_histogram(binwidth = 500, fill = "blue", color = "black", alpha = 0.7) +
   labs(title = "Dist. of Keyword Popularity",
       subtitle = "Average shares of average keywords",
       x = "Avg. Shares of Avg. Keywords",
       y = "Count of Articles") +
  theme_minimal()
```

#### Figure 2: Initial MLR Model

```{r}
#| label: initial-linear-reg 

mlr_model <- lm(log(shares) ~ kw_avg_avg + n_tokens_content + 
               data_channel + day_published + global_subjectivity +
               title_sentiment_polarity, 
               data = newsdf)
 tidy(mlr_model) |>
   kable(digits = 4)

```

#### Figure 3: MLR Residual Plot

```{r}
#| label: MLR-residual-plot 

plot(mlr_model, which = c(1), 
      main = "Residuals vs Fitted Values for MLR Model",
      sub = "Non-random pattern suggests violation of linear assumption",
      col = "darkblue")
```

#### Figure 4: Initial Logistic Regression Table

```{r}
#| label: initial-log-table 

logistic_model <- glm(is_viral ~ kw_avg_avg +
                        log(n_tokens_content + 0.0001) +
                        data_channel +
                        day_published + 
                        global_subjectivity +
                        title_sentiment_polarity, 
                family = binomial(link = "logit"),
                data = newsdf)

tidy(logistic_model) |>
  mutate(
    estimate = round(estimate, 4),
    std.error = round(std.error, 4),
    statistic = round(statistic, 4),
    p.value = round(p.value, 4)
  ) %>%
  kable(digits = 4, align = "lrrrr",
        col.names = c("Term", "Estimate", "Std.Error", "z-statistic", "p-value"))
```

#### Figure 5: Final Logistic Model Table

```{r}
#| label: log-interacts 

tidy(interactsMod) |>
  kable(digits = 3)
```

#### Figure 6: Final Logistic Model Metrics

```{r}
#| label: model-info 

TN <- 9960
FP <- 5518
FN <- 5569
TP <- 10364

accuracy <- (TP + TN) / (TP + TN + FP + FN)
misclassification <- (FP + FN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)  # Recall
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
FPR <- FP / (FP + TN)
FNR <- FN / (FN + TP)


metrics_table <- data.frame(
  Metric = c("Accuracy", "Misclassification Rate", "Sensitivity (Recall)", "Specificity", 
             "Precision", "False Positive Rate (FPR)", "False Negative Rate (FNR)"),
  Value = c(accuracy, misclassification, sensitivity, specificity, 
            precision, FPR, FNR)
)

kable(metrics_table, digits = 3, caption = "Logistic Model Metrics Summary")
```

```{r}
#| label: select-threshold 

target_sensitivity <- 0.65

#interation model only
roc_data_int <- aug_interact |>
  roc_curve(is_viral, probability, event_level = "second")

closest_point <- roc_data_int |>
  mutate(diff = abs(sensitivity - target_sensitivity)) |>
  arrange(diff) |> 
  slice(1)

# Display selected threshold
optimal_threshold <- closest_point$.threshold
cat("Optimal threshold for classification:", round(optimal_threshold, 3))
```
