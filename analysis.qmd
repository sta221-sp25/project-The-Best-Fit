```{r}
library(pROC)
library(tidyverse)
library(tidymodels)
library(ggplot2)

library(dplyr)
library(gridExtra)
library(knitr)
if (!requireNamespace("car", quietly = TRUE)) {
  install.packages("car")
}
library(car)

newsdf <- read_csv("data/OnlineNewsPopularity.csv")

newsdf$day_published <- NA  # Create a new empty column

newsdf$day_published <- case_when(
  newsdf$weekday_is_monday == 1 ~ "Monday",
  newsdf$weekday_is_tuesday == 1 ~ "Tuesday",
  newsdf$weekday_is_wednesday == 1 ~ "Wednesday",
  newsdf$weekday_is_thursday == 1 ~ "Thursday",
  newsdf$weekday_is_friday == 1 ~ "Friday",
  newsdf$weekday_is_saturday == 1 ~ "Saturday",
  newsdf$weekday_is_sunday == 1 ~ "Sunday",
)

newsdf$day_published <- factor(
  newsdf$day_published,
  levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
)


newsdf$data_channel <- NA

newsdf$data_channel <- case_when(
  newsdf$data_channel_is_entertainment == 1 ~ "Entertainment",
  newsdf$data_channel_is_bus == 1 ~ "Business",
  newsdf$data_channel_is_socmed == 1 ~ "Social Media",
  newsdf$data_channel_is_tech == 1 ~ "Technology",
  newsdf$data_channel_is_world == 1 ~ "World",
)

newsdf$data_channel <- factor(newsdf$data_channel)

newsdf <- newsdf %>%
  drop_na(kw_avg_avg, kw_max_avg, kw_min_avg, n_tokens_content, 
          data_channel, day_published, global_subjectivity, 
          title_sentiment_polarity)
```

**Analysis + peer review**

**Draft report**

**Introduction and data**

Our project utilizes the University of California Irvine Machine Learning Repository’s  “Online News Popularity” data set. It includes share counts and descriptive characteristics for articles published by Mashable over two years (from 2013 to 2015). Mashable Inc. is a digital media website founded in 2005 and as of November 2015, it has over 6,000,000 Twitter followers and over 3,200,000 fans on Facebook. The data set in total, has 39644 observations, each representing an individual article. Our project motivation is that with the rise of the internet, we're interested in seeing how different factors influence or decide what goes "viral" on social media. For media companies specifically, this could aid in revealing patterns in what attracts readers to certain articles. Thus, our project aims to answer the

**Key Research Question**:

How do different article attributes (ex. Polarity, Positive/Negative Sentiments, Number of Images, etc.) relate to its virality on social media?

**Key Variables:** 

**rate_positive_words** - Rate of positive words among non-neutral tokens in the article content. Values range from 0.0 to 1.0, with a mean of 0.6822 and standard deviation of 0.1902. This metric captures the positive emotional tone of the article.

**Rate_negative_words** - Rate of negative words among non-neutral tokens in the article content. Values range from 0.0 to 1.0, with a mean of 0.2879 and standard deviation of 0.1562. This metric captures the negative emotional tone of the article.

**title_sentiment_polarity** - Measure of the title's sentiment polarity (positivity/negativity). Values range from -1.0 (extremely negative) to 1.0 (extremely positive), with a mean of 0.0714 and standard deviation of 0.2654. This indicates how emotionally charged article titles are.

**n_tokens_content** - Number of words in the article content. Values range from 0 to 8,474 words, with a mean of 546.51 and standard deviation of 471.10. This quantifies the overall length of the article.

**n_tokens_title** - Number of words in the article title. Values range from 2 to 23 words, with a mean of 10.40 and standard deviation of 2.11. This measures length of headlines.

**data_channel** - Categorical variable denoting article topic, merged from indicators: data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, and data_channel_is_world. This classifies content by subject area.

**day_published** - Categorical variable indicating publication day, merged from indicators: weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday. Additionally includes is_weekend (mean 0.1309) to distinguish weekday from weekend publications.

**kw_avg_avg** - Average shares of average keywords in the article. Values range from 0.0 to 43,567.66, with a mean of 3,135.86 and standard deviation of 1,318.13. This measures the expected popularity of the article's keyword selection.

**global_subjectivity** - Measures the overall subjectivity of the article text. Values range from 0.0 (completely objective) to 1.0 (completely subjective), with a mean of 0.4434 and standard deviation of 0.1167. This quantifies how opinion-based versus fact-based the content is.

**title_sentiment_polarity:** A measure of the title's sentiment polarity (positivity/negativity). Values range from -1.0 (extremely negative) to 1.0 (extremely positive), with a mean of 0.0714 and a standard deviation of 0.2654.

**Key EDA**

Response Variable - our initial EDA of the response variable revealed that it had a heavily right skewed, unimodal distribution. Thus, we imposed a log transformation, which was more symmetric and normally distributed.

```{r}
#| label: initial-response 

ggplot(newsdf, aes(x = shares)) +
  geom_histogram(binwidth = 500, fill = "blue", color = "black", alpha = 0.7) +
  scale_x_continuous(limits = c(0, 50000)) +  # Limit for better visualization
  labs(title = "Distribution of Article Shares",
       x = "Number of Shares",
       y = "Count") +
  theme_minimal()
```

```{r}
#| label: log-transform-response 

ggplot(newsdf, aes(x = log(shares))) +  
  geom_histogram(binwidth = 0.2, fill = "purple", color = "black", alpha = 0.7) +
  labs(title = "Log-Transformed Distribution of Shares",
       x = "Log(Shares)",
       y = "Count") +
  theme_minimal()
```

Key Variables - The key predictor variables we found from our initial exploration were Data Channel and Day Published, with the bivariate EDA we performed with our response variable, log(shares), shown below.

```{r}
#| label: distribution-day-pub

news_summary <- newsdf |>
  group_by(day_published) |> 
  summarize(mean_shares = mean(shares)) 

head(news_summary)
news_summary|>
  ggplot(aes(x = day_published, y = mean_shares)) +
  geom_bar(stat = "identity", fill = "skyblue") + 
  labs(
    x = "Day Published", 
    y = "Mean Number of Article Shares",
    title = "Mean Number of Article Shares vs. Day Published"
  )
```

```{r}
#| label: data-channel-explore

news_summary_channel <- newsdf |>
  group_by(data_channel) |> 
  summarize(mean_shares = mean(shares)) 

head(news_summary_channel)
news_summary_channel |>
  drop_na(data_channel) |>
  ggplot(aes(x = data_channel, y = mean_shares)) +
  geom_bar(stat = "identity", fill = "skyblue") + 
  labs(
    x = "Data Channel", 
    y = "Mean Number of Article Shares",
    title = "Mean Number of Article Shares vs. Data Channel"
  )
```

#### Methodology

From our EDA, we found that the most significant predictor variables were data_channel and day_published, however, other predictors we initially viewed (such as n_tokens_title, etc.) were not significant. Thus, we chose to fit an initial MLR model using data_channel and day_published, alongside other predictors that we hadn't attempted before (such as kw_avg_avg, n_tokens_content, etc.).

This produced said results:

```{r}
#| label: MLR-initial 

mlr_model <- lm(log(shares) ~ kw_avg_avg + n_tokens_content + 
              data_channel + day_published + global_subjectivity +
              title_sentiment_polarity, 
              data = newsdf)
tidy(mlr_model) |>
  kable(digits = 4)

summary_result <- summary(mlr_model)

rmse_val <- sqrt(mean(summary_result$residuals^2))

results_table <- data.frame(
  Metric = c("R^2", "Adjusted R-squared", "RMSE"),
  Value = c(
    round(summary_result$r.squared, 4),
    round(summary_result$adj.r.squared, 4),
    round(rmse_val, 4)
  )
)

results_table
```

We found that all our predictors were statistically significant (p-value \< 0.05), with the exception of day_publishedFriday, and thus kept them in this model.

```{r}
#| label: MLR-res

summary_result <- summary(mlr_model)

rmse_val <- sqrt(mean(summary_result$residuals^2))

results_table <- data.frame(
  Metric = c("R^2", "Adjusted R-squared", "RMSE"),
  Value = c(
    round(summary_result$r.squared, 4),
    round(summary_result$adj.r.squared, 4),
    round(rmse_val, 4)
  )
)

results_table
```

```{r}
#| label: residuals-plot

plot(mlr_model, which = c(1))
```

Our MLR had an $RMSE$ of 0.8303 and $R^2$ value of 0.0903 . Considering that our response variable was log(shares), this suggests that this MLR model is poorly fit to our data, as it can only explain about 9.03% of the variability that we can see from our response variable, log(shares), and moreover, has a very high RMSE value. This is reaffirmed by the residuals plot, which reveals that the residuals are not randomly distributed, meaning that linear regression might not be the ideal model as our data does not satisfy the linear regression criteria. 

```{r}
#| label: log-model 

newsdf$is_viral <- ifelse(newsdf$shares >= 1400, 1, 0)


logistic_model <- glm(is_viral ~ kw_avg_avg  + n_tokens_content + data_channel +
                  day_published + global_subjectivity +
                  title_sentiment_polarity, 
                family = binomial(link = "logit"),
                data = newsdf)
```

Thus, for our final model, we pivoted to use a logistic regression model to classify an article as popular or otherwise. We selected a threshold for popularity of "1400 shares" based on prior literature, transforming shares column into a binary response variable with 1 for articles more popular than 1400 shares and 0 for those with less.

As for predictors, we continued using statistically significant predictors data_channel,day_published, etc.

```{r}
tidy(logistic_model) |>
kable(digits= 4)
```

We can clearly see that all but one of the variables remain statistically significant predictors, barring day_publishedTuesday and day_publishedThursday.

```{r}
#| label: plot-roc-curve 
pred_prob <- predict.glm(logistic_model, type = "response")

model_aug <- augment(logistic_model)

model_aug <- model_aug |>
  mutate(is_viral = as.factor(is_viral))

model_aug <- model_aug |>
  bind_cols(pred_prob = pred_prob)
# calculate sensitivity and specificity at each threshold
roc_curve_data <- model_aug |>
roc_curve(is_viral, pred_prob,
event_level = "second")
# plot roc curve
autoplot(roc_curve_data)

```

```{r}
#| label: select-threshold

target_sensitivity <- 0.65

closest_point <- roc_curve_data |>
  mutate(diff = abs(sensitivity - target_sensitivity)) |>
  arrange(diff) |> 
  slice(1)

closest_point$.threshold

```

```{r}
#| label: confusion-matrix

log_odds <- predict(logistic_model, newsdf)
newsdf <- newsdf |>
  bind_cols(log_odds = log_odds)
newsdf <- newsdf |>
  mutate(
    predict_prob = exp(log_odds) / (1 + exp(log_odds))
  )

newsdf <- newsdf |>
  mutate(
    estimateVirality = case_when(
      predict_prob < 0.4916522 ~ 0,
      predict_prob > 0.4916522 ~ 1,
      TRUE ~ NA_real_  
    )
  )

newsdf <- newsdf |> 
  mutate(
         estimateVirality = as.factor(estimateVirality),
         is_viral = as.factor(is_viral)
         )

model_conf <- newsdf |>
  conf_mat(is_viral,estimateVirality)

autoplot(model_conf, type="heatmap")

```

```{r}
#| label: model-info 


roc_obj <- roc(newsdf$is_viral, newsdf$predict_prob)
auc_value <- auc(roc_obj)

TN <- 9977
FP <- 5501
FN <- 5577
TP <- 10356


accuracy <- (TP + TN) / (TP + TN + FP + FN)
misclassification <- (FP + FN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)  
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
FPR <- FP / (FP + TN)
FNR <- FN / (FN + TP)

metrics_table <- data.frame(
  Metric = c("Accuracy", "Misclassification Rate", "Sensitivity (Recall)", "Specificity", 
             "Precision", "False Positive Rate (FPR)", "False Negative Rate (FNR)", "AUC"),
  Value = c(accuracy, misclassification, sensitivity, specificity, 
            precision, FPR, FNR, auc_value)
)

# Print table
kable(metrics_table, digits = 3, caption = "Logistic Model Metrics Summary")
```

```{r}
vif_results <- vif(logistic_model)
print(vif_results)  
```

While our logistic regression model's ROC curve has an AUC of about 0.693, this is a dramatic improvement from our initial linear regression model, and thus we chose to continue using our logistic model for this data set.

#### Results

The final model we fitted was:

$$
\begin{aligned}
\text{logit}(p_{isViral}) &= -1.3623 \\& +\ 0.0004 \times \text{kwAvgAvg} \\& +\ 0.0003 \times \text{nTokensContent} \\& -\ 0.6580 \times \text{dataChannelEntertainment} \\& +\ 0.8882 \times \text{dataChannelSocialMedia} \\& +\ 0.4839 \times \text{dataChannelTechnology} \\& -\ 0.5048 \times \text{dataChannelWorld} \\& -\ 0.1092 \times \text{dayPublishedTuesday} \\& -\ 0.1194 \times \text{dayPublishedWednesday} \\& +\ 0.1450 \times \text{dayPublishedFriday} \\& +\ 1.0247 \times \text{dayPublishedSaturday} \\& +\ 0.8979 \times \text{dayPublishedSunday} \\& +\ 0.5384 \times \text{globalSubjectivity} \\& +\ 0.2244 \times \text{titleSentimentPolarity}
\end{aligned}
$$

Our logistic regression model has an AUC of around 0.693, an accuracy of 0.647, specificity of 0.645, and sensitivity of 0.650. In comparison, the mis-classification rate, FPR, and FNR rates were 0.353, 0.355, and 0.350 respectively. This suggests that our model is moderately well fit for the data, as while the accuracy, specificity, sensitivity, and precision were relatively high at around 0.650, the FNR, FPR, and mis-classification rates were lower, at around 0.350.

From our model, we can conclude that \_\_\_ \# ADD INTERPRETATIONS OF ODDS AND YEAH
