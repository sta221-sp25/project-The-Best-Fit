---
title: "Analysis Written Report"
author: "The BEST Fit - Philip, Olivia, Leo, Allison"
date: "4/10/2025"
format: pdf
execute: 
  warning: false
  message: false
  echo: false
editor: visual
---

```{r}
#| label: load-pkg-data

library(pROC)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(knitr)
if (!requireNamespace("car", quietly = TRUE)) {
  install.packages("car")
}
library(car)

newsdf <- read_csv("data/OnlineNewsPopularity.csv")
```

## **Analysis + Peer Review**

[**Draft report**]{.underline}

[**Introduction**]{.underline}

The ways in which people interact with media and discover news have dramatically shifted in recent years, with social media often displacing traditional news outlets. The decentralized nature of social media means the reach of each article is largely dependent on its individual merits, rather than the popularity of the publication it belongs to.

Thus a question arises: What article attributes are associated with social media virality?

In this report we will investigate the effects of different article features on social media success using the University of California Irvine Machine Learning Repository’s “Online News Popularity” data set. It includes share counts and descriptive characteristics for articles published by Mashable, a digital media website, over two years (from 2013 to 2015). The data has 39,644 entries in total, with each repsenting an individual article and its associated textual and metadata features.

**Key Variables:** 

**rate_positive_words** - Rate of positive words among non-neutral tokens in the article content. Values range from 0.0 to 1.0, with a mean of 0.6822 and standard deviation of 0.1902. This metric captures the positive emotional tone of the article.

**Rate_negative_words** - Rate of negative words among non-neutral tokens in the article content. Values range from 0.0 to 1.0, with a mean of 0.2879 and standard deviation of 0.1562. This metric captures the negative emotional tone of the article.

**title_sentiment_polarity** - Measure of the title's sentiment polarity (positivity/negativity). Values range from -1.0 (extremely negative) to 1.0 (extremely positive), with a mean of 0.0714 and standard deviation of 0.2654. This indicates how emotionally charged article titles are.

**n_tokens_content** - Number of words in the article content. Values range from 0 to 8,474 words, with a mean of 546.51 and standard deviation of 471.10. This quantifies the overall length of the article.

**n_tokens_title** - Number of words in the article title. Values range from 2 to 23 words, with a mean of 10.40 and standard deviation of 2.11. This measures length of headlines.

**data_channel** - Categorical variable denoting article topic, merged from indicators: data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, and data_channel_is_world. This classifies content by subject area.

**day_published** - Categorical variable indicating publication day, merged from indicators: weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday, weekday_is_saturday, weekday_is_sunday. Additionally includes is_weekend (mean 0.1309) to distinguish weekday from weekend publications.

**kw_avg_avg** - Average shares of average keywords in the article. Values range from 0.0 to 43,567.66, with a mean of 3,135.86 and standard deviation of 1,318.13. This measures the expected popularity of the article's keyword selection.

**global_subjectivity** - Measures the overall subjectivity of the article text. Values range from 0.0 (completely objective) to 1.0 (completely subjective), with a mean of 0.4434 and standard deviation of 0.1167. This quantifies how opinion-based versus fact-based the content is.

### **Key EDA**

Response Variable - our initial EDA of the response variable revealed that it had a heavily right skewed, unimodal distribution. Thus, we imposed a log transformation, which was more symmetric and normally distributed.

```{r}
#| label: initial-response 

ggplot(newsdf, aes(x = shares)) +
  geom_histogram(binwidth = 500, fill = "blue", color = "black", alpha = 0.7) +
  scale_x_continuous(limits = c(0, 50000)) +  # Limit for better visualization
   labs(title = "Distribution of Article Shares",
       subtitle = "Raw Shares Count (Limited to 50,000)",
       x = "Number of Shares",
       y = "Count of Articles") +
  theme_minimal()
```

```{r}
#| label: log-transform-response 

ggplot(newsdf, aes(x = log(shares))) +  
  geom_histogram(binwidth = 0.2, fill = "purple", color = "black", alpha = 0.7) +
  labs(title = "Log-Transformed Dist. of Article Shares",
       subtitle = "More normal distribution post-transformation",
       x = "Log(Shares)",
       y = "Count of Articles")+
  theme_minimal()
```

Key Variables - The key predictor variables we found from our initial exploration were Data Channel and Day Published, with the bivariate EDA we performed with our response variable, log(shares), shown below.

[Data Cleaning]{.underline}

(Combining existing weekday and data_channel indicator variables into their respective categorical variables.)

```{r}
newsdf$day_published <- NA  # Create a new empty column

newsdf$day_published <- case_when(
  newsdf$weekday_is_monday == 1 ~ "Monday",
  newsdf$weekday_is_tuesday == 1 ~ "Tuesday",
  newsdf$weekday_is_wednesday == 1 ~ "Wednesday",
  newsdf$weekday_is_thursday == 1 ~ "Thursday",
  newsdf$weekday_is_friday == 1 ~ "Friday",
  newsdf$weekday_is_saturday == 1 ~ "Saturday",
  newsdf$weekday_is_sunday == 1 ~ "Sunday",
)

newsdf$day_published <- factor(
  newsdf$day_published,
  levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
)


newsdf$data_channel <- NA

newsdf$data_channel <- case_when(
  newsdf$data_channel_is_entertainment == 1 ~ "Entertainment",
  newsdf$data_channel_is_bus == 1 ~ "Business",
  newsdf$data_channel_is_socmed == 1 ~ "Social Media",
  newsdf$data_channel_is_tech == 1 ~ "Technology",
  newsdf$data_channel_is_world == 1 ~ "World",
)

newsdf$data_channel <- factor(newsdf$data_channel)


tab <- newsdf %>%
  select(url, day_published, data_channel) %>%
  head(5)
kable(tab, caption = "Transformed Data")
  
```

```{r}
#| label: distribution-day-pub

news_summary <- newsdf |>
  group_by(day_published) |> 
  summarize(mean_shares = mean(shares)) 

head(news_summary)
news_summary|>
  ggplot(aes(x = day_published, y = mean_shares)) +
  geom_bar(stat = "identity", fill = "skyblue") + 
  labs(
    title = "Mean Article Shares by Day of Publication", 
    subtitle = "Weekend articles tend to receive more shares",
    x = "Day Published", 
    y = "Mean Number of Shares",
  ) 
```

```{r}
#| label: data-channel-explore

news_summary_channel <- newsdf |>
  group_by(data_channel) |> 
  summarize(mean_shares = mean(shares)) 

head(news_summary_channel)
news_summary_channel |>
  drop_na(data_channel) |>
  ggplot(aes(x = data_channel, y = mean_shares)) +
  geom_bar(stat = "identity", fill = "skyblue") + 
  labs(
    title = "Mean Article Shares by Content Category", 
    subtitle = "Social Media articles more popular than other catagories",
    x = "Content Category", 
    y = "Mean Number of Shares",
  ) 
```

### Methodology

Since initial EDA revealed a heavy right skew in the distribution of article shares, as well as a poential non-linear relationship, we elected to use a logistic regression model with a transformed binary response variable.

Based on our initial EDA and empirical logit visualization, we selected data channel, day published, article subjectivity, title sentiment polarity, log transformed 'avg keyword popularity', and log transformed article content length to fit an initial logistic model.

For the response variable, we constructed "is_viral" by transforming 'share' count into a binary response variable, with 1 for articles more popular than 1400 shares and 0 for those with less. We selected this threshold of 1400 shares based on prior literature\[CITE HERE\] and the recommendation of the data set curator.
## Model Specification

$$ 
\begin{aligned}
\text{logit}(P(\text{is\_viral} = 1)) &= \beta_0 \\
&+ \beta_1 \times \log(\text{kw\_avg\_avg} + 0.0001) \\
&+ \beta_2 \times \log(\text{n\_tokens\_content} + 0.0001) \\
&+ \beta_3 \times \text{data\_channel} \\
&+ \beta_4 \times \text{day\_published} \\
&+ \beta_5 \times \text{global\_subjectivity} \\
&+ \beta_6 \times \text{title\_sentiment\_polarity}
\end{aligned}
$$
```{r}
#| label: log-model 

newsdf$is_viral <- ifelse(newsdf$shares >= 1400, 1, 0)


logistic_model <- glm(is_viral ~ log(kw_avg_avg + 0.0001)  + log(n_tokens_content + 0.0001) + data_channel +
                  day_published + global_subjectivity +
                  title_sentiment_polarity, 
                family = binomial(link = "logit"),
                data = newsdf)

tidy(logistic_model) |>
  mutate(
    estimate = round(estimate, 4),
    std.error = round(std.error, 4),
    statistic = round(statistic, 4),
    p.value = round(p.value, 4)
  ) %>%
  kable(digits = 4, align = "lrrrr",
        col.names = c("Term", "Estimate", "Std.Error", "z-statistic", "p-value"))
```

Our initial fit gives all of the predictors significant p-values (p<0.05) and most predictors relatively high magnitude z-statistics, indicating that all variables in the model have statistically significant relationships with the likelihood of content going viral.


## Coeficent Analysis

When fitting our model, we also visualized the adjusted odds ratios to ensure that all predictors were statistically significant.

```{r}
#| label: coefficient-estimates 

model_odds_ratios <- tidy(logistic_model, exponentiate = TRUE, conf.int = TRUE)

ggplot(data = model_odds_ratios, aes(x = term, y = estimate)) +
  geom_point() +
  geom_hline(yintercept = 1, lty = 2) + 
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high))+
  labs(title = "Adjusted odds ratios",
       x = "",
       y = "Estimated AOR") +
  coord_flip()
```

From this initial visualization, none of the 95% confidence intervals for our predictor coefficients included 1, suggesting that they were all statistically significant. While we had a couple predictors whose confidence intervals were close to 1, we decided to still keep them in the model as day_published_Thursday is one of the factors of the day_published variable, and thus it's acceptable that some of the levels for day_published were not necessarily significant because the other levels were. To add, we decided to keep log(n_tokens_content) as we both saw a possible interaction effect in the earlier EDA and we felt that it was at least a valuable predictor to consider in our model.


## Interaction Effects
Next, we considered the addition of poetential interaction effects between article length and data channel, and between global subjectivity and data channel.
The hypothesis for this experiment were:

$$
H_o : B_{n-tokens-content *data-channel } =0 \\ H_A: B_{n-tokens-content *data-channel } \neq0 
$$

```{r}
#| echo: false
#| message: false
#| warning: false

nullMod <- glm(is_viral ~ log(kw_avg_avg+0.0001) + log(n_tokens_content+0.0001) + data_channel +
                 day_published + global_subjectivity +
                 title_sentiment_polarity, 
               family = binomial(link = "logit"),
               data = newsdf)

interactsMod <- glm(is_viral ~ log(kw_avg_avg+0.0001) + log(n_tokens_content+0.0001) + data_channel +
                     day_published + global_subjectivity +
                     title_sentiment_polarity + n_tokens_content * data_channel + 
                      global_subjectivity * data_channel, 
                   family = binomial(link = "logit"),
                   data = newsdf)

# Extract log-likelihoods
L_0 <- glance(nullMod)$logLik
L_a <- glance(interactsMod)$logLik

# Calculate test statistic
G <- -2 * (L_0 - L_a)

# Calculate p-value
p_value <- pchisq(G, df = 5, lower.tail = FALSE)

# Create table for drop in deviance test
deviance_table <- tibble(
  Model = c("Null Model", "Interaction Model"),
  `Log-Likelihood` = c(L_0, L_a),
  `Deviance Statistic (G)` = c(NA, G),
  df = c(NA, 5),
  `p-value` = c(NA, p_value)
)

# Display table using knitr::kable
knitr::kable(deviance_table, 
             caption = "Drop in Deviance Test Results",
             digits = c(0, 3, 3, 0, 4))

```


Examining the output of the  deviance test, the p-value is very low, at around 0.
This indicates that the data provides sufficient evidence that at-least one of the newly added interaction terms is a statistically significant predictor in whether an article will go viral or not, after accounting for data channel, day published, global subjectivity, title sentiment polarity, average key word popularity, and main body length for a given article. Therefore, we will keep the interaction effects in the final model.

## Model Evaluation and Comparison

```{r}
#| label: model-evaluation
#| fig.width: 10
#| fig.height: 5

# Function to prepare ROC data
prepare_roc_data <- function(model, data, model_name) {
  # Generate predicted probabilities
  probs <- predict(model, type = "response")
  
  # Augment model with predictions
  aug_data <- augment(model) |>
    mutate(
      is_viral = as.factor(.data$is_viral),
      probability = probs,
      model = model_name
    )
  
  return(aug_data)
}

# Prepare data for both models
aug_null <- prepare_roc_data(nullMod, newsdf, "Initial Model")
aug_interact <- prepare_roc_data(interactsMod, newsdf, "Interaction Model")

# Combine data for comparison
combined_aug <- bind_rows(aug_null, aug_interact)

# Generate ROC curves
roc_data <- combined_aug |>
  group_by(model) |>
  roc_curve(is_viral, probability, event_level = "second")

# Calculate AUC values
# Calculate AUC for each model separately to avoid group_by issues
auc_null <- roc_auc(
  data = filter(combined_aug, model == "Initial Model"),
  truth = is_viral,
  probability,
  event_level = "second"
)

auc_interact <- roc_auc(
  data = filter(combined_aug, model == "Interaction Model"),
  truth = is_viral,
  probability,
  event_level = "second"
)

# Combine AUC values
auc_values <- bind_rows(
  mutate(auc_null, model = "Initial Model"),
  mutate(auc_interact, model = "Interaction Model")
)

# Create separate ROC curve plots and arrange them in a grid
# Generate ROC data for each model separately
roc_data_null <- aug_null |>
  roc_curve(is_viral, probability, event_level = "second")

roc_data_interact <- aug_interact |>
  roc_curve(is_viral, probability, event_level = "second")

# Plot for Initial Model
plot_null <- ggplot(roc_data_null, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.2, color = "#E41A1C") +
  geom_abline(lty = 2, alpha = 0.5, slope = 1, intercept = 0) +
  coord_equal() +
  labs(
    title = "Initial Model",
    subtitle = paste("AUC =", round(filter(auc_values, model == "Initial Model")$.estimate, 4)),
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()

# Plot for Interaction Model
plot_interact <- ggplot(roc_data_interact, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.2, color = "#4DAF4A") +
  geom_abline(lty = 2, alpha = 0.5, slope = 1, intercept = 0) +
  coord_equal() +
  labs(
    title = "Interaction Model",
    subtitle = paste("AUC =", round(filter(auc_values, model == "Interaction Model")$.estimate, 4)),
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal()

# Arrange plots side by side using gridExtra
library(gridExtra)
grid.arrange(plot_null, plot_interact, 
             ncol = 2,
             top = "ROC Curves Comparison")

# Display AUC values
kable(auc_values, 
      caption = "AUC Values for Both Models",
      digits = 4)
```




From the ROC curves, we can see that 
 1) Both models have an ROC curve above the random threshold, approaching the top left corner, indicating some predictive power in classifying an article
 2) The  Interaction Model (AUC = 0.6902) demonstrates marginally better predictive performance than the Initial Model (AUC = 0.681), confirming our belief that the interaction effects are meaningful predictors.
 3) Based on the curve, the optimal threshold for our model should target  sensitivity ~ 0.65. 
 
Selecting the point closest to the ROC curve to sensitivity 0.65 yields a threshold of approximately 0.464.

```{r}
#| label: threshold-selection

target_sensitivity <- 0.65

#interation model only
roc_data_int <- aug_interact |>
  roc_curve(is_viral, probability, event_level = "second")

closest_point <- roc_data_int |>
  mutate(diff = abs(sensitivity - target_sensitivity)) |>
  arrange(diff) |> 
  slice(1)

# Display selected threshold
optimal_threshold <- closest_point$.threshold
cat("Optimal threshold for classification:", round(optimal_threshold, 3))
```



Model Performance

```{r}
#| label: roc-curve-interaction 
#| echo: false

# Generate predicted probabilities for ROC analysis
prob_roc_int <- predict(interactsMod, type = "response")
# Augment model and add probabilities
aug_data_int <- augment(interactsMod) |>
  mutate(
    is_viral = as.factor(is_viral),
    prob_roc_int = prob_roc_int
  )
# Generate ROC data
roc_data_int <- aug_data_int |>
  roc_curve(is_viral, prob_roc_int, event_level = "second")
# Plot ROC curve
autoplot(roc_data_int)

# Calculate AUC
auc_result_int <- roc_auc(
  data = aug_data_int,
  truth = is_viral,
  prob_roc_int,
  event_level = "second"
)
```

```{r}
#| label: select-threshold-interaction 

target_sensitivity <- 0.65

closest_point <- roc_data_int |>
  mutate(diff = abs(sensitivity - target_sensitivity)) |>
  arrange(diff) |> 
  slice(1)

closest_point$.threshold
```

```{r}
#| echo: false
#| fig.cap: "Confusion Matrix for the Interaction Model"

# Predict log-odds using the interaction model
log_odds_int <- predict(interactsMod, newdata = newsdf)
news_eval <- newsdf |>
  bind_cols(log_odds_int = log_odds_int)
# Convert to predicted probabilities
news_eval <- news_eval |>
  mutate(
    prob_int = exp(log_odds_int) / (1 + exp(log_odds_int))
  )
news_eval <- news_eval |>
  mutate(
    predicted_label = case_when(
      prob_int < 0.481 ~ 0,
      prob_int >= 0.481 ~ 1,
      TRUE ~ NA_real_
    ),
    predicted_label = as.factor(predicted_label),
    is_viral = as.factor(is_viral)
  )
conf_matrix_int <- news_eval |>
  conf_mat(is_viral, predicted_label)
autoplot(conf_matrix_int, type = "heatmap")
```

```{r}
auc_result_int <- roc_auc(
  data = aug_data_int,
  truth = is_viral,
  prob_roc_int,
  event_level = "second"
)
auc_result_int
```

### Results

The final model we fitted was:

$$
\begin{aligned}
\text{logit}(p_{isViral}) &= -1.3623 \\& +\ 0.0004 \times \text{kwAvgAvg} \\& +\ 0.0003 \times \text{nTokensContent} \\& -\ 0.6580 \times \text{dataChannelEntertainment} \\& +\ 0.8882 \times \text{dataChannelSocialMedia} \\& +\ 0.4839 \times \text{dataChannelTechnology} \\& -\ 0.5048 \times \text{dataChannelWorld} \\& -\ 0.1092 \times \text{dayPublishedTuesday} \\& -\ 0.1194 \times \text{dayPublishedWednesday} \\& +\ 0.1450 \times \text{dayPublishedFriday} \\& +\ 1.0247 \times \text{dayPublishedSaturday} \\& +\ 0.8979 \times \text{dayPublishedSunday} \\& +\ 0.5384 \times \text{globalSubjectivity} \\& +\ 0.2244 \times \text{titleSentimentPolarity}
\end{aligned}
$$

```{r}
#| label: model-info 

TN <- 9993
FP <- 5576
FN <- 5485
TP <- 10357

accuracy <- (TP + TN) / (TP + TN + FP + FN)
misclassification <- (FP + FN) / (TP + TN + FP + FN)
sensitivity <- TP / (TP + FN)  # Recall
specificity <- TN / (TN + FP)
precision <- TP / (TP + FP)
FPR <- FP / (FP + TN)
FNR <- FN / (FN + TP)


metrics_table <- data.frame(
  Metric = c("Accuracy", "Misclassification Rate", "Sensitivity (Recall)", "Specificity", 
             "Precision", "False Positive Rate (FPR)", "False Negative Rate (FNR)", "AUC"),
  Value = c(accuracy, misclassification, sensitivity, specificity, 
            precision, FPR, FNR, auc_result_int$.estimate)
)

kable(metrics_table, digits = 3, caption = "Logistic Model Metrics Summary")
```

```{r}
vif_results <- vif(logistic_model)
print(vif_results)  
```

Our logistic regression model has an AUC of around 0.693, an accuracy of 0.647, specificity of 0.645, and sensitivity of 0.650. In comparison, the mis-classification rate, FPR, and FNR rates were 0.353, 0.355, and 0.350 respectively. This suggests that our model is moderately well fit for the data, as while the accuracy, specificity, sensitivity, and precision were relatively high at around 0.650, the FNR, FPR, and mis-classification rates were lower, at around 0.350. This precision means that approximately 65% of articles predicted to be viral were correctly classified, indicating the model performs significantly better than random chance. This relatively low predictive power may also be due to random noise, as many features of each article are likely uncaptured by the dataset and article virality may be influenced by sudden trends.

Some of our initial interpretations when we were first getting started were that a linear model would be the right fit. We also predicted that variables such as the rate of positive words, rate of negative words, and the number of words in the article title (n_tokens_title) would have a significant effect on article virality. However, in reality, we found that a linear model would not be the best fit, and also many of the initial variables we suspected to be significant turned out to not be significant. Rather, some of the more unexpected variables turned out to be better predictors.

From our model, we can conclude that several key factors significantly influence article virality: Content category plays a critical role in determining article popularity. Notably, Social media articles and technology articles are approximately 2.43 and 1.62 times, respectively, more likely than a similar business article to go viral. Conversely, Entertainment and World news are less successful categories, with 48.2% and 39.6% lower odds of going viral, respectively.

This suggests that readers are particularly engaged with content about social media and technology innovations, while being less likely to widely share entertainment and world news. Day of publication is another important factor in article virality. Weekend publications dramatically outperform weekday content, with Saturday articles enjoying 2.79 times higher odds and Sunday articles 2.45 times higher odds of virality compared to Monday publications. This weekend effect likely arises from increased leisure time as people take off from work or school, as weekdays show the opposite effect, with Tuesday and Wednesday’s articles being 10.4% and 11.3% less likely to be viral.

While our initial set of “article sentiment” variables had relatively low predictive power, our model predicted a purely subjective article would have 71.3% higher odds of achieving viral status compared to purely objective content. Similarly, an article with purely positive sentiment would have 25.2% higher odds than a similar neutral title. This trend shows that, overall, more emotionally charged and polarizing content tends to be shared more often than neutral reporting.

Finally, while statistically important to the model, article length and keyword popularity have relatively small coefficients, making them less important to practical cases.

### **Appendix**
```{r}
#| label: confusion-matrix

log_odds <- predict(logistic_model, newsdf)
newsdf <- newsdf |>
  bind_cols(log_odds = log_odds)
newsdf <- newsdf |>
  mutate(
    predict_prob = exp(log_odds) / (1 + exp(log_odds))
  )

newsdf <- newsdf |>
  mutate(
    estimateVirality = case_when(
      predict_prob < 0.4916522 ~ 0,
      predict_prob > 0.4916522 ~ 1,
      TRUE ~ NA_real_  
    )
  )

newsdf <- newsdf |> 
  mutate(
         estimateVirality = as.factor(estimateVirality),
         is_viral = as.factor(is_viral)
         )

model_conf <- newsdf |>
  conf_mat(is_viral,estimateVirality)

autoplot(model_conf, type="heatmap")

```

**Exploratory Data Analysis:**

**Data Set Description**: 

Our project utilizes the University of California Irvine Machine Learning Repository’s  “Online News Popularity” data set. It includes share counts and descriptive characteristics for articles published by Mashable over two years (from 2013 to 2015). Mashable Inc. is a digital media website founded in 2005 and as of November 2015, it has over 6,000,000 Twitter followers and over 3,200,000 fans on Facebook. The data set in total, has 39644 observations, each representing an individual article. Observations include characteristics such as: Number of Words in Title/Content, Rate of Unique Words, Number of Images, Data Channel, Day Published, Rate of Positive/Negative Words, Polarity, etc.  Our intention is to use the data set to predict the number of shares/virality of an article based on different variables. 

**Key Variables**: 

rate_positive_words -  rate of positive words among non-neutral tokens, which captures how emotionally charged the language is. 

Rate_negative_words - rate of negative words among non-neutral tokens, which captures how emotionally charged the language is. 

title_sentiment_polarity - A measure of how polarizing the title is

N_tokens_content - A measure of how long the article’s content is 

N_tokens_title -  A measure of how long the article title is

data_channel - a categorical variable denoting article topic merged from: Data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, and data_channel_is_world.

day_published- a categorical variable indicating publication day merged from indicators: Weekday_is_monday, weekday_is_tuesday, weekday_is_wednesday, weekday_is_thursday, weekday_is_friday,weekday_is_saturday, weekday_is_sunday 

\break

[Data Cleaning]{.underline}

First we have to combine the existing weekday and data_channel indicator variables into their respective categorical variables.

```{r}
newsdf$day_published <- NA  # Create a new empty column

newsdf$day_published <- case_when(
  newsdf$weekday_is_monday == 1 ~ "Monday",
  newsdf$weekday_is_tuesday == 1 ~ "Tuesday",
  newsdf$weekday_is_wednesday == 1 ~ "Wednesday",
  newsdf$weekday_is_thursday == 1 ~ "Thursday",
  newsdf$weekday_is_friday == 1 ~ "Friday",
  newsdf$weekday_is_saturday == 1 ~ "Saturday",
  newsdf$weekday_is_sunday == 1 ~ "Sunday",
)

newsdf$day_published <- factor(
  newsdf$day_published,
  levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
)


newsdf$data_channel <- NA

newsdf$data_channel <- case_when(
  newsdf$data_channel_is_entertainment == 1 ~ "Entertainment",
  newsdf$data_channel_is_bus == 1 ~ "Business",
  newsdf$data_channel_is_socmed == 1 ~ "Social Media",
  newsdf$data_channel_is_tech == 1 ~ "Technology",
  newsdf$data_channel_is_world == 1 ~ "World",
)

newsdf$data_channel <- factor(newsdf$data_channel)


tab <- newsdf %>%
  select(url, day_published, data_channel) %>%
  head(5)
kable(tab, caption = "Transformed Data")
  
```

```{r}
ggplot(data =newsdf, aes(x=data_channel))+
  geom_bar(fill = "green")+
  labs(
    x ="Data Channel",
    y= "Relative Frequency", 
    title = "Distribution of article catagories"
  )
```

Next, we discovered that approx 8,233 articles are not tagged for a specific data channel. Due to the nature of the dataset, it's unclear if this was because the article was simply missing a tag, it was mis-tagged while being collected, or if it simply doesn't belong in any of these categories. With the relatively large size of our dataset, we decided to exclude entries lacking a data tag NA's from our data channel analysis altogether. These articles lacking a data channel were filtered out.

```{r}
newsdf<- newsdf |> 
  filter(!is.na(data_channel))
```

[Response Variable/Univariate EDA]{.underline}

```{r}
#| label: initial-explore

ggplot(newsdf, aes(x = shares)) +
  geom_histogram(binwidth = 500, fill = "blue", color = "black", alpha = 0.7) +
  scale_x_continuous(limits = c(0, 50000)) +  # Limit for better visualization
  labs(title = "Distribution of Article Shares",
       x = "Number of Shares",
       y = "Count") +
  theme_minimal()
```

```{r}
newsdf |>
  summarize(
    mean_shares = mean(shares),
    median_shares = median(shares),
    sd_shares = sd(shares),
    min_shares = min(shares),
    max_shares = max(shares),
    q1 = quantile(shares, 0.25),
    q3 = quantile(shares, 0.75)
  )
```

The distribution of \# of shares is highly right skewed, a median of 1400 shares and a few highly shared articles. Notably, the mean of 2878 shares is far larger

```{r}
#| label: zoom-in-response

ggplot(newsdf, aes(x = shares)) +
  geom_histogram(binwidth = 500, fill = "blue", color = "black", alpha = 0.7) +
  scale_x_continuous(limits = c(0, 10000)) +  # Zoom in on the first 10,000 shares
  labs(title = "Distribution of Article Shares (Zoomed In)",
       x = "Number of Shares",
       y = "Count") +
  theme_minimal()

```

To make deal with this strong right skew, we applied a log transformation to the share variable, yielding a less skewed distribution.

```{r}
#| label: log-transform

ggplot(newsdf, aes(x = log(shares))) +  
  geom_histogram(binwidth = 0.2, fill = "purple", color = "black", alpha = 0.7) +
  labs(title = "Log-Transformed Distribution of Shares",
       x = "Log(Shares)",
       y = "Count") +
  theme_minimal()
```

[Predictor Variable/Univariate EDA]{.underline}

```{r}
themeA <- theme_minimal() +
  theme(axis.title.y = element_blank())  

a1 <- ggplot(newsdf, aes(x = rate_positive_words)) +
  geom_histogram(binwidth = 0.02, fill = "green", color = "black") +
  labs(title = "Positive Words Rate",
       x = "Rate of positive words") + 
  themeA

a2 <- ggplot(newsdf, aes(x = rate_negative_words)) +
  geom_histogram(binwidth = 0.02, fill = "red", color = "black") +
  labs(title = "Negative Words Rate",
       x = "Rate of negative words") + 
  themeA

a3 <- ggplot(newsdf, aes(x = title_sentiment_polarity)) +
  geom_histogram(binwidth = 0.02, fill = "blue", color = "black") +
  labs(title = "Title Polarity",
       x = "Polarity score") + 
  themeA

a4 <- ggplot(newsdf, aes(x = n_tokens_content)) +
  geom_histogram(binwidth = 100, fill = "purple", color = "black") +
  labs(title = "Article Length",
       x = "Number of words") + 
  themeA

a5 <- ggplot(newsdf, aes(x = n_tokens_title)) +
  geom_histogram(binwidth = 1, fill = "pink", color = "black") +
  labs(title = "Title Length",
       x = "Number of words") + 
  themeA

grid.arrange(a1, a2, a3, a4, a5, ncol = 3, 
             left = "Frequency", 
             top = "Distribution of Article Features")
```

Examining the rate of positive words in an article, we see a left skew distribution, with modes at \~0, \~0.75 and \~1. The median positivity is approx 0.71 positivity rate, and the range is from 0 to 1.

```{r}
ggplot(newsdf, aes(x = rate_positive_words)) +
  geom_histogram(binwidth = 0.02, fill = "green", color = "black") +
  labs(title = "Rel. Freq of Positive Words Rate",
       x = "Rate of positive words",
       y= "Frequency")
```

```{r}
newsdf |>
  summarize(
    mean= mean(rate_positive_words),
    median = median(rate_positive_words),
    std.dev = sd(rate_positive_words),
    min = min(rate_positive_words),
    max = max(rate_positive_words),
  )
```

The rate of negative words shows the opposite trend, with a slight right skew. Similarly, there seems to be a second mode at 0 negativity. The median is approx 0.28 negativity rate, with an approximately equal mean.

```{r}
ggplot(newsdf, aes(x = rate_negative_words)) +
  geom_histogram(binwidth = 0.02, fill = "red", color = "black") +
  labs(title = "Rel Freq of Negative Words Rate",
       x = "Rate of negative words",
       y= "Frequency")
```

```{r}
newsdf |>
  summarize(
    mean= mean(rate_negative_words),
    median = median(rate_negative_words),
    std.dev = sd(rate_negative_words),
    min = min(rate_negative_words),
    max = max(rate_negative_words),
  )
```

```{r}
ggplot(newsdf, aes(x = n_tokens_content)) +
  geom_histogram(binwidth = 100, fill = "purple", color = "black") +
  labs(title = "Rel. Freq. of Article Length",
       x = "Number of words",
       y= "frequency")

```

```{r}
newsdf |>
  summarize(
    mean= mean(n_tokens_content),
    median = median(n_tokens_content),
    std.dev = sd(n_tokens_content),
    min = min(n_tokens_content),
    max = max(n_tokens_content),
  )
```

For article length, the graph shows a strongly right-skewed distribution, with a median length of 444 and a high standard deviation of 477 words. To remedy this, we might consider a log transformation which yields a more even distribution.

```{r}
ggplot(newsdf, aes(x = log(n_tokens_content))) +
  geom_histogram( fill = "purple", color = "black") +
  labs(title = "Rel. Freq of Log transformed Article Length",
       x = "log of # of words",
       y= "frequency")
```

For the number of tokens in the title, we can see a highly symetric distribution centered at 10, with a standard deviation of 2.14.

```{r}
ggplot(newsdf, aes(x = n_tokens_title)) +
  geom_histogram(binwidth = 1, fill = "pink", color = "black") +
  labs(title = "Rel. Freq. of Title Length",
       x = "Number of words",
       y= "frequency")
```

```{r}
newsdf |>
  summarize(
    mean= mean(n_tokens_title),
    median = median(n_tokens_title),
    std.dev = sd(n_tokens_title),
    min = min(n_tokens_title),
    max = max(n_tokens_title),
  )
```

Finally, our initial EDA of title polarity found a massive frequency spike at 0 frequency, which might correspond to failed measurements or the vast majority of our articles not presenting signifigant title polarity.

```{r}
ggplot(newsdf, aes(x = title_sentiment_polarity)) +
  geom_histogram(binwidth = 0.02, fill = "blue", color = "black") +
  labs(title = "Title Polarity",
       x = "Polarity score",
       y= "Frequency") 
```

To fix this issue and for ease of use, we catagorized articles into negative, neutral and positive polarity, with a threshold of 0 +- 0.05 for neutral. Most titles remain neutral, and there are more positive than negative headlines.

```{r}
newsdf$title_sentiment_category <- cut(newsdf$title_sentiment_polarity,
                                       breaks = c(-Inf, -0.05, 0.05, Inf),
                                       labels = c("Negative", "Neutral", "Positive"))
ggplot(newsdf, aes(x = title_sentiment_category, fill = title_sentiment_category)) +
  geom_bar() +
  scale_fill_manual(values = c("Negative" = "red", "Neutral" = "gray", "Positive" = "blue")) +
  labs(title = "Distribution of Title Sentiment Polarity",
       x = "Sentiment Category",
       y = "Count") +
  theme_minimal()
```

[Bi-variate EDA]{.underline}

```{r}
p1 <- ggplot(newsdf, aes(x = rate_positive_words, y = log(shares))) +
  geom_point( color = "blue") +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Log(Shares) vs Rate of Positive Words",
       x = "Rate of Positive Words", 
       y = "Log(Shares)") +
  theme_minimal()

p2 <- ggplot(newsdf, aes(x = rate_negative_words, y = log(shares))) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Log(Shares) vs Rate of Negative Words",
       x = "Rate of Negative Words", 
       y = "Log(Shares)")
grid.arrange(p1, p2, ncol = 2)
```

By plotting the rate of positive words and the rate of negative words against the log transformed share, we can see that neither have a particularly strong relationship with how often the article is shared. The rate of positive words seems to have a weak positive relationship with shares, and the rate of negative words seems to have a weak negative relationship, but both have significant outliers are 1 and 0.

```{r}
ggplot(newsdf, aes(x = title_sentiment_category, y = (shares), fill = title_sentiment_category)) +
  stat_summary(fun = "mean", geom = "bar") +
  scale_fill_manual(values = c("Negative" = "red", "Neutral" = "gray", "Positive" = "blue")) +
  labs(title = "Average Shares by Title Polarity",
       x = "Title Polarity Category",
       y = "Average Shares") +
  theme(legend.position = "none")
```

In contrast, there seems to be some relationship between title polarity and the number of shares, with positive polarity associated with greater share counts.

Next,

```{r}
f1<- ggplot(newsdf, aes(x = (n_tokens_content), y = log(shares))) +
  geom_point( color = "violet") +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Log(Shares) vs Article Length",
       x = "Article Length(Words)", 
       y = "Log(Shares)") +
  theme_minimal()

f2<- ggplot(newsdf, aes(x = log(n_tokens_content), y = log(shares))) +
  geom_point( color = "violet") +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Log(Shares) vs Article Length",
       x = "log(Article Length(Words))", 
       y = "Log(Shares)") +
  theme_minimal()

grid.arrange(f1,f2,ncol=2)
```

These graphs show a weak, positive relationship between article length and the log transformed number of shares. Due to the skew, we can apply the log transform to the article length. This shows a more even distribution, with no clear relationship.

Similarly, there doesn't seem to be any clear relationship between title length and the number of shares in the graph below.

```{r}
 ggplot(newsdf, aes(x = (n_tokens_title), y = log(shares))) +
  geom_point( color = "gold") +
  geom_smooth(method = "lm", color = "blue") +
  labs(title = "Log(Shares) vs Title Length",
       x = "Title Length(Words)", 
       y = "Log(Shares)") +
  theme_minimal()


```

```{r}
#| label: distribution-day 

news_summary <- newsdf |>
  group_by(day_published) |> 
  summarize(mean_shares = mean(shares)) 

head(news_summary)
news_summary|>
  ggplot(aes(x = day_published, y = mean_shares)) +
  geom_bar(stat = "identity", fill = "skyblue") + 
  labs(
    x = "Day Published", 
    y = "Mean Number of Article Shares",
    title = "Mean Number of Article Shares vs. Day Published"
  )

```

```{r}
model <- lm(log(shares) ~ rate_negative_words + rate_positive_words, data =newsdf ) 
  tidy(model)|>
  kable(digits=3)

```

```{r}
vif_values <- vif(model)
print("Variance Inflation Factors:")
print(vif_values)
```

This visualization and summarization suggests that the day an article is published does not have a significant impact on the virality of an article, as the mean number of article shares do not differ much between days. Therefore, this predictor may not be as important as others when it comes to predicting the virality of an article.

```{r}
#| label: data-channel-vis 

news_summary_channel <- newsdf |>
  group_by(data_channel) |> 
  summarize(mean_shares = mean(shares)) 

head(news_summary_channel)
news_summary_channel |>
  drop_na(data_channel) |>
  ggplot(aes(x = data_channel, y = mean_shares)) +
  geom_bar(stat = "identity", fill = "skyblue") + 
  labs(
    x = "Data Channel", 
    y = "Mean Number of Article Shares",
    title = "Mean Number of Article Shares vs. Data Channel"
  )
```

From our visualizations, it appears that articles that are in the social media data channel perform the best in terms of average number of shares (\~3600), while articles that are in the World data channel perform the worst (\~2250). Business, Entertainment, and Technology articles all seem to receive a mean of about 3000 shares. However, from our earlier univariate analysis, we saw that the Social Media Data Channel has the lowest number of articles, and thus there is a possibility that any outliers for this data channel category would have a larger impact in skewing the mean.

[Interaction Effects Exploration]{.underline}

```{r}
#| label: model-interact

# Base model without interaction
model1 <- lm(log(shares) ~ n_tokens_content + n_tokens_title, data = newsdf)

# Model with interaction effect
model2 <- lm(log(shares) ~ n_tokens_content + n_tokens_title +
         	n_tokens_content:n_tokens_title, data = newsdf)

# Create tidy dataframes of results with model labels
summary_model1 <- tidy(model1) %>% mutate(Model = "Model 1")
summary_model2 <- tidy(model2) %>% mutate(Model = "Model 2")

# Display combined coefficient table
kable(rbind(summary_model1, summary_model2),
  	digits = 3,
  	col.names = c("Term", "Estimate", "Std. Error", "t value", "p-value", "Model"),
  	caption = "Regression Coefficients for Both Models")

# Compare models using ANOVA
model_comparison <- tidy(anova(model1, model2))
kable(model_comparison, digits = 3, caption = "ANOVA Comparison of Models")

```

When comparing linear models with the title and article length as predictors, we explored to see if an interaction effect would have a meaningful difference. Our results show that the effect of title length depends on article length. IE, for very short articles, we'd expect longer titles to be beneficial and for longer articles, vice versa.

```{r}
#| label: interact-channel
news_summary <- newsdf |>
  drop_na(data_channel) |>
  group_by(data_channel, title_sentiment_category) |>
  summarize(mean_shares = mean(shares, na.rm = TRUE))

news_summary |>
  ggplot(aes(x = title_sentiment_category, y = mean_shares)) + 
  geom_bar(stat = "identity", fill = "skyblue") + 
  facet_wrap(~ data_channel) +
  labs(
    title = "Mean Shares for Title Sentiment Categories",
    subtitle = "Faceted by Data Channel Type",
    x = "Title Sentiment"
  )
```

From this visualization, it's suggested that the title sentiment may have different impacts on the mean number of shares depending on the type of data channel, thus suggesting that there may be a statistically significant interaction effect between the title sentiment and the data channel type. For instance, while for Social Media and Entertainment, it appears that articles with a Negative sentiment have the greatest number of mean shares, for World and Technology articles, Positive sentiment articles had the greatest mean shares, and for Business, Neutral sentiment articles had the greatest number.

```{r}
#| label: news-fit

news_fit <- lm(log(shares) ~ data_channel + title_sentiment_category + data_channel * title_sentiment_category, data = newsdf)

tidy(news_fit) |>
  kable(digits = 3)
```

However, when looking at the actual fitted model, all the interaction terms between the data channel and the sentiment category have large p-values (greater than 0.05) suggesting that none of the interaction terms are actually statistically significant between data channel and title sentiment category. In the future we could consider looking at the mean of the shares rather than the log, as it appears as though there may be interaction effects for the mean but not the log of the shares.

```{r}
#| label: more-interaction

title_fit_mod <- lm(log(shares) ~ n_tokens_title + data_channel * n_tokens_title + data_channel, data = newsdf) 
tidy(title_fit_mod) |>
  kable(digits = 3)
```

From this model, we found that while the data channel type at times has a statistically significant linear relationship to log(shares), specifically for Social Media, Technology, and World, the number of words in the title does not have a linear relationship to log(shares), based on their respective p-values. For instance, the p-value for n_tokens_title is 0.677, thus suggesting that there is not a statistically significant linear relationship between n_tokens_title and log(shares). However, while the number of words in the title does not have a significant linear relationship with log(shares) directly, its interaction term specifically with when the data channel is World, is significant (as shown by the p-value of 0.003).
